{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukbbX2aUZPId"
      },
      "source": [
        "# Problema:\n",
        "Analiza el texto Moby Dick de Herman Melville 1851, que está en el corpus gutenberg incluido en nltk (puedes descargalo de Poliformat: melville-moby_dyck.txt).\n",
        "Calcula los siguientes resultados, después de eliminar signos de puntuación y stopwords:\n",
        "\n",
        "*   Cantidad total de tokens.\n",
        "*   Número medio de tokens por frase.\n",
        "*   Cantidad total de tokens diferentes (vocabulario).\n",
        "*   Cantidad total de stems diferentes.\n",
        "*   Muestre los 10 stems más frecuentes con su frecuencia.\n",
        "\n",
        "Indique también cuál es el nombre, el verbo y el adjetivo más frecuente en el corpus.\n",
        "\n",
        "Tendrá que utilizar las siguientes herramientas: un segmentador de oraciones, un tokenizador, una lista de stopwords, un stemmer y un POS tagger.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhY72PrxZPIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54dcd958-25a4-4ae4-d574-4049879c85e3"
      },
      "source": [
        "#Hecho por César Martínez Chico\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import PorterStemmer, sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "from nltk.probability import FreqDist"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "moby_dick = \"melville-moby_dick.txt\"\n",
        "with open(moby_dick, 'r') as archivo:\n",
        "    moby_dick_or = archivo.read().lower()\n",
        "    patron_puntuacion = r\"[^\\w\\s]\"\n",
        "    moby_dick = re.sub(patron_puntuacion, \"\", moby_dick_or)\n",
        "\n",
        "    #a) cantidad de tokens\n",
        "    tokens = word_tokenize(moby_dick)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokensNoSw = [word for word in tokens if word.lower() not in stop_words]\n",
        "    tokensCantidad = len(tokensNoSw)\n",
        "    print(\"Cantidad de tokens en total: \" + str(tokensCantidad))\n",
        "\n",
        "    #b) cantidad de tokens medio por frase\n",
        "    frases = sent_tokenize(moby_dick_or)\n",
        "    tokensFrase = tokensCantidad / len(frases)\n",
        "    print(\"Cantidad de tokens media por frase: \" + str(tokensFrase))\n",
        "\n",
        "    #c)cantidad de tokens diferente, vocabulario\n",
        "    tokensUnicos = set(tokensNoSw)\n",
        "    print(\"La cantidad de tokens unicos es: \" + str(len(tokensUnicos)))\n",
        "\n",
        "    #d)Cantidad total de stems diferentes.\n",
        "    stemmer = PorterStemmer()\n",
        "    stems = [stemmer.stem(token) for token in tokensNoSw]\n",
        "    stemsUnicos = len(set(stems))\n",
        "    print(\"La cantidad de stems unicos es: \" + str(stemsUnicos))\n",
        "\n",
        "    #e) muestre los stems más frecuentes por frecuencia\n",
        "    frecuencia_stems = FreqDist(stems)\n",
        "    stems_mas_frecuentes = frecuencia_stems.most_common(10)\n",
        "    print(\"Los stems mas frecuentes son: \")\n",
        "    for stem, frecuencia in stems_mas_frecuentes:\n",
        "      print(stem, frecuencia)\n",
        "\n",
        "    #Indique también cuál es el nombre, el verbo y el adjetivo más frecuente en el corpus.\n",
        "    frecuencia_pos = FreqDist(tag for word, tag in nltk.pos_tag(tokensUnicos))\n",
        "    pos_tags = nltk.pos_tag(tokensNoSw)\n",
        "    nombre = [word for word, tag in pos_tags if tag.startswith('NN')]\n",
        "    verbo = [word for word, tag in pos_tags if tag.startswith('VB')]\n",
        "    adjetivo = [word for word, tag in pos_tags if tag.startswith('JJ')]\n",
        "    nombre = FreqDist(nombre).max()\n",
        "    verbo = FreqDist(verbo).max()\n",
        "    adjetivo = FreqDist(adjetivo).max()\n",
        "    print(\"El nombre, verbo y adjetivo mas comun son: \" + str(nombre) + \"  \" + str(verbo) + \"  \" + str(adjetivo))\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7be5zQA-dyTU",
        "outputId": "423fb32e-0812-4acc-a571-208913b7786b"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cantidad de tokens en total: 108692\n",
            "Cantidad de tokens media por frase: 11.066177967827326\n",
            "La cantidad de tokens unicos es: 19832\n",
            "La cantidad de stems unicos es: 13493\n",
            "Los stems mas frecuentes son: \n",
            "whale 1458\n",
            "one 919\n",
            "like 589\n",
            "upon 565\n",
            "ship 555\n",
            "man 499\n",
            "ahab 492\n",
            "ye 486\n",
            "sea 466\n",
            "seem 459\n",
            "El nombre, verbo y adjetivo mas comun son: whale  said  whale\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DxcKsx71gkP4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}